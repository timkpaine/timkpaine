{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabe215e-cf89-48fb-b227-a384be00f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import VBox, HBox\n",
    "from pyspark.sql import SparkSession\n",
    "from perspective import PerspectiveWidget\n",
    "\n",
    "from helpers.data import (\n",
    "    HOST,\n",
    "    MACHINES_PORT,\n",
    "    USAGE_PORT,\n",
    "    STATUS_PORT,\n",
    "    JOBS_PORT,\n",
    ")\n",
    "from helpers.spark import (\n",
    "    MACHINE_SCHEMA,\n",
    "    MACHINE_SCHEMA_SPARK,\n",
    "    USAGE_SCHEMA,\n",
    "    USAGE_SCHEMA_SPARK,\n",
    "    STATUS_SCHEMA,\n",
    "    STATUS_SCHEMA_SPARK,\n",
    "    JOBS_SCHEMA,\n",
    "    JOBS_SCHEMA_SPARK,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4aa82d-9e60-4419-8fc7-47c2697c0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important imports\n",
    "from helpers.spark import (\n",
    "    get_df_from_server,\n",
    "    push_to_perspective,\n",
    ")\n",
    "from helpers.fastapi import (\n",
    "    perspective_spark_bridge,\n",
    "    start_server,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f808e2d3-29bc-413f-b6e4-5b33c7322904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 15:35:29 WARN Utils: Your hostname, mbp-m1 resolves to a loopback address: 127.0.0.1; using 10.0.1.10 instead (on interface en0)\n",
      "24/06/07 15:35:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/07 15:35:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/07 15:35:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Perspective Demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2938e11-ad08-455a-aad7-4398e3b6f338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 15:35:30 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "24/06/07 15:35:31 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "24/06/07 15:35:31 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "24/06/07 15:35:31 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "# Get spark streaming dfs\n",
    "machines_df = get_df_from_server(spark, MACHINE_SCHEMA_SPARK, HOST, MACHINES_PORT)\n",
    "usage_df = get_df_from_server(spark, USAGE_SCHEMA_SPARK, HOST, USAGE_PORT)\n",
    "status_df = get_df_from_server(spark, STATUS_SCHEMA_SPARK, HOST, STATUS_PORT)\n",
    "jobs_df = get_df_from_server(spark, JOBS_SCHEMA_SPARK, HOST, JOBS_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d64d516-7285-46ec-9c72-31e8985d5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct 4 separate perspective widgets. Each will have its own table internally\n",
    "machines_widget = PerspectiveWidget(MACHINE_SCHEMA, index=\"machine_id\", settings=False)\n",
    "usage_widget = PerspectiveWidget(USAGE_SCHEMA, index=\"machine_id\", settings=False)\n",
    "status_widget = PerspectiveWidget(STATUS_SCHEMA, index=\"machine_id\", sort=[[\"last_update\", \"desc\"]], settings=False)\n",
    "jobs_widget = PerspectiveWidget(JOBS_SCHEMA, sort=[[\"start_time\", \"desc\"]], settings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad0165a-a8cb-4cbd-895f-631e21392e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc5fdf6299f4a8c950df3b79f480046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(PerspectiveWidget(columns=['machine_id', 'kind', 'cores', 'region', 'zone'], setâ€¦"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a little bit of layout with ipywidgets\n",
    "VBox(children=[\n",
    "    HBox(children=[machines_widget, usage_widget]),\n",
    "    HBox(children=[status_widget, jobs_widget]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f949043b-00e1-4fd8-956c-3047f09ff294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:Listening on http://localhost:50154\n",
      "INFO:     Started server process [6872]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:50154 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "app = perspective_spark_bridge(\n",
    "    [\"machines\"],\n",
    "    [machines_widget],\n",
    ")\n",
    "port = start_server(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d55e4ab-3737-4f71-b590-a95e9686481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 14:59:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/pm/0nt85wh51h39f604mgmyyfwm0000gn/T/temporary-1ffec542-273e-4b0b-be94-2e2de1a69c7f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/07 14:59:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/06/07 14:59:13 WARN TextSocketMicroBatchStream: Stream closed by localhost:8083\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "push_to_perspective(machines_df, \"machines\", \"localhost\", port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e859237-4be3-4994-83b1-78e3eaea19d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
